{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"1VqfWAea5AwZFZRWmjXbEojbGXplBUxRO","authorship_tag":"ABX9TyMOk34k0vN28OoYvcJLJjiA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## model.py"],"metadata":{"id":"d9obGM05sBtx"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            # bias false because batch norm will cancel it out so no use for that\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels), # Batch norm is not in original paper, but could be beneficial\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),  # Batch norm is not in original paper, but could be beneficial\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class UNET(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n","        super().__init__()\n","\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # down part of UNET\n","\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","\n","        # up part of UNET\n","\n","        for feature in reversed(features):\n","            self.ups.append(\n","                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n","            )\n","            self.ups.append(DoubleConv(feature*2, feature))\n","\n","        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","\n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","\n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2]\n","\n","            if x.shape != skip_connection.shape:\n","                # different solutions can be made for examp padding\n","                x = TF.resize(x, size=skip_connection.shape[2:])\n","\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            x = self.ups[idx+1](concat_skip)\n","\n","        return self.final_conv(x)"],"metadata":{"id":"FuBCkLE5sBOw","executionInfo":{"status":"ok","timestamp":1688320032423,"user_tz":-180,"elapsed":3301,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## dataset.py"],"metadata":{"id":"hLFhUkJesOdD"}},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","\n","class CarvanaDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","        mask[mask == 255.0] = 1\n","\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","\n","        return image, mask"],"metadata":{"id":"WTVA74S8sY6V","executionInfo":{"status":"ok","timestamp":1688320032424,"user_tz":-180,"elapsed":7,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## utils.py"],"metadata":{"id":"JU6pVsozsju0"}},{"cell_type":"code","source":["# from dataset import CarvanaDataset\n","from torch.utils.data import DataLoader\n","import torch\n","import torchvision\n","\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","\n","def get_loaders(train_dir, train_mask_dir, val_dir, val_mask_dir, batch_size, train_transform, val_transform,num_worker=4, pin_memory=True):\n","    train_ds = CarvanaDataset(image_dir=train_dir, mask_dir=train_mask_dir, transform=train_transform)\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_worker, pin_memory=pin_memory, shuffle=True)\n","    val_ds = CarvanaDataset(image_dir=val_dir, mask_dir=val_mask_dir, transform=val_transform)\n","    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_worker, pin_memory=pin_memory, shuffle=False)\n","\n","    return train_loader, val_loader\n","\n","\n","def check_accuracy(loader, model, device=\"cuda\"):  # for binary results\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(preds)\n","            dice_score += (2 * (preds * y).sum()) / (preds + y).sum() + 1e-8\n","\n","    print(f\"Got {num_correct} / {num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n","    print(f\"Dice score: {dice_score/len(loader)}\")\n","\n","    model.train()\n","\n","\n","def save_predictions_as_imgs(loader, model, folder=\"saved_images\", device=\"cuda\"):\n","    model.eval()\n","\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device)\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","\n","        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n","        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}\")\n","\n","    model.train()\n"],"metadata":{"id":"r75QzW3-sjim","executionInfo":{"status":"ok","timestamp":1688323393171,"user_tz":-180,"elapsed":13,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## train.py"],"metadata":{"id":"kzCNxopssrB5"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"VH0j2SzxGbGt","executionInfo":{"status":"error","timestamp":1688323532811,"user_tz":-180,"elapsed":107167,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"68ddf8bc-9345-4417-a16e-ba2133b4234d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 315/315 [01:42<00:00,  3.06it/s, loss=0.108]\n"]},{"output_type":"stream","name":"stdout","text":["=> Saving checkpoint\n","Got 1798654 / 1843200 with acc 97.58\n","Dice score: 0.9493903517723083\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2219\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m                 \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: ''","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-49c2952cedbf>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-49c2952cedbf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0msave_predictions_as_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"saved_images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-49465fe11d18>\u001b[0m in \u001b[0;36msave_predictions_as_imgs\u001b[0;34m(loader, model, folder, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{folder}/pred_{idx}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2220\u001b[0m                 \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"unknown file extension: {ext}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSAVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: unknown file extension: "]}],"source":["from torch.nn.parallel.data_parallel import data_parallel\n","import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","# from model import UNET\n","\n","# from utils import (\n","#     load_checkpoint,\n","#     save_checkpoint,\n","#     get_loaders,\n","#     check_accuracy,\n","#     save_predictions_as_imgs\n","# )\n","\n","LEARNING_RATE = 1e-4\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 3\n","NUM_WORKERS = 2\n","IMAGE_HEIGHT = 160  # kaggle dataset original = 1280\n","IMAGE_WIDTH = 240  # kaggle dataset original = 1918\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","data = \"/content/drive/MyDrive/Colab_Notebooks/Pytorch/UNET/data\"\n","TRAIN_IMG_DIR = f\"{data}/train_images\"\n","TRAIN_MASK_DIR = f\"{data}/train_masks\"\n","VAL_IMG_DIR = f\"{data}/val_images\"\n","VAL_MASK_DIR = f\"{data}/val_masks\"\n","\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader)\n","\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(DEVICE)\n","        targets = targets.float().unsqueeze(1).to(DEVICE)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss=loss.item())\n","\n","\n","def main():\n","    train_transform = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Rotate(limit=35, p=1.0),\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.1),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    val_transform = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n","    loss_fn = nn.BCEWithLogitsLoss()  # if out_channels would be 3, than cross entropy would be used\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    train_loader, val_loader = get_loaders(\n","        TRAIN_IMG_DIR,\n","        TRAIN_MASK_DIR,\n","        VAL_IMG_DIR,\n","        VAL_MASK_DIR,\n","        BATCH_SIZE,\n","        train_transform,\n","        val_transform,\n","        NUM_EPOCHS,\n","        PIN_MEMORY\n","    )\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","\n","    for epoc in range(NUM_EPOCHS):\n","        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","\n","        checpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict()\n","        }\n","        save_checkpoint(checpoint)\n","\n","        check_accuracy(val_loader, model, device=DEVICE)\n","\n","        save_predictions_as_imgs(val_loader, model, folder=\"saved_images\", device=DEVICE)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}